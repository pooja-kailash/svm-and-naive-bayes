{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q.1.\n",
        "\n",
        "Ans\n",
        "\n",
        "   - What is Information Gain?\n",
        "\n",
        "It is a metric used in machine learning to determine which feature is most effective for splitting data at a given node in a decision tree.\n",
        "\n",
        "It is calculated as the difference between the entropy of the dataset before the split and the weighted average of the entropy after the split.\n",
        "\n",
        "A higher Information Gain means the feature is more effective at reducing uncertainty and creating more pure, homogeneous subsets of data.\n",
        "\n",
        "   - How it's used in Decision Trees\n",
        "\n",
        "1. Root Node Selection: The algorithm calculates the Information Gain for every feature in the dataset and chooses the one with the highest gain to be the root node.\n",
        "\n",
        "2. Splitting and Branching: The dataset is split into subsets based on the values of the chosen feature. This process creates child nodes.\n",
        "\n",
        "3. Recursive Selection: The process is repeated for each new child node. The algorithm evaluates the remaining features to find the one with the highest Information Gain for that specific subset of data and splits it again.\n",
        "\n",
        "4. Stopping Condition: The algorithm continues to split nodes until a stopping condition is met, such as reaching a maximum depth, having nodes with very low entropy (pure nodes), or when no features remain to split on.\n",
        "\n",
        "5. Classification/Prediction: Once the tree is built, it can be used for classification. To classify a new data point, you traverse the tree from the root down to a leaf node, following the branches based on the features of the data point. The leaf node then provides the predicted class."
      ],
      "metadata": {
        "id": "grJkGH9V57L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2.\n",
        "\n",
        "Ans\n",
        "\n",
        "Both Gini Impurity and Entropy are measures of impurity used in decision tree algorithms to evaluate how well a feature separates the classes in a dataset. They help in selecting the best attribute for splitting at each node of the tree.\n",
        "\n",
        "\n",
        "    1. Gini impurity\n",
        "Definition:\n",
        "Gini Impurity measures the degree of impurity or uncertainty in a dataset. It represents the probability that a randomly chosen element would be incorrectly classified if it were randomly labeled according to the class distribution in the node.\n",
        "\n",
        "   2. 2. Entropy\n",
        "\n",
        "Definition:\n",
        "Entropy is a measure of impurity or randomness in the data. It is derived from information theory and indicates the amount of information needed to describe the outcome of a random variable.\n",
        "\n",
        "- Strengths and weaknesses\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "    Strengths:\n",
        "\n",
        "- Faster and more efficient: Its simpler calculation is ideal for larger datasets or real-time applications where speed is a priority.\n",
        "\n",
        "- Simplicity: The formula is easier to understand and calculate, as it does not involve logarithms.\n",
        "\n",
        "- Good for balanced classes: Works well and is the preferred metric when classes in the dataset are relatively balanced.\n",
        "\n",
        "    Weaknesses:\n",
        "\n",
        "- Less sensitive to skewed distributions: It is less responsive to subtle changes in probability distribution compared to entropy.\n",
        "- Less robust: It can be less robust than entropy in some cases.\n",
        "\n",
        "\n",
        "2. Entropy\n",
        "\n",
        "    Strengths:\n",
        "\n",
        "- Theoretically richer: As a concept from information theory, it provides a more robust and nuanced measure of information and uncertainty.\n",
        "\n",
        "- Good for imbalanced classes: Its sensitivity to class distribution makes it a good choice for datasets where class labels are skewed.\n",
        "\n",
        "- Produces deeper trees: Can lead to more granular and precise splits, potentially resulting in deeper decision trees.\n",
        "\n",
        "    Weaknesses:\n",
        "\n",
        "- Computationally slower: The logarithmic function makes it more intensive to calculate, especially on large datasets.\n",
        "\n",
        "- Similar performance in practice: For most cases, the difference in accuracy between trees built with Gini Impurity and Entropy is minimal, so the additional computation time may not be justified.\n",
        "\n",
        "    Appropriate use cases\n",
        "\n",
        "1. Use Gini Impurity if:\n",
        "\n",
        "- Computational speed is critical, as in real-time systems or with very large datasets.\n",
        "\n",
        "- The classes in your dataset are well-balanced.\n",
        "\n",
        "2. Use Entropy if:\n",
        "\n",
        "- You are working with a highly imbalanced dataset and want to be more sensitive to finer splits.\n",
        "- You are prioritizing the theoretical foundation of information gain and have a smaller dataset where computation time is not a major concern."
      ],
      "metadata": {
        "id": "HFZcUoIM57Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3.\n",
        "\n",
        "Ans\n",
        "\n",
        "Pre-Pruning in Decision Trees\n",
        "\n",
        "Definition:\n",
        "\n",
        "Pre-pruning, also known as early stopping, is a technique used to prevent overfitting in decision tree algorithms. It involves stopping the growth of the tree early, before it perfectly classifies all the training examples.\n",
        "\n",
        "Instead of allowing the tree to grow fully and then pruning it afterward, pre-pruning imposes constraints during the tree-building process to decide when to stop splitting a node.\n",
        "\n",
        "Common Stopping Criteria for Pre-Pruning:\n",
        "\n",
        "- Maximum Depth:\n",
        "Stop splitting when the tree reaches a specified maximum depth.\n",
        "\n",
        "- Minimum Samples per Node:\n",
        "Stop splitting if a node contains fewer samples than a predefined threshold.\n",
        "\n",
        "- Minimum Information Gain / Gini Decrease:\n",
        "Stop splitting if the reduction in impurity (or gain in information) is below a threshold.\n",
        "\n",
        "- Maximum Number of Leaf Nodes:\n",
        "Limit the total number of leaf nodes in the tree.\n",
        "\n",
        "- Chi-Square Test:\n",
        "Stop splitting if the split is not statistically significant."
      ],
      "metadata": {
        "id": "_ddv-Xin57F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4."
      ],
      "metadata": {
        "id": "GWPY1QUNFiQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset (Iris dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train (fit) the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Evaluate the model accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy on Test Data: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZYKUHnIFtoz",
        "outputId": "670139e5-3ba3-492a-d12a-4e615bdcfe8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy on Test Data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5.\n",
        "\n",
        "Ans\n",
        "\n",
        "\n",
        "    Support Vector Machine (SVM)\n",
        "\n",
        "Definition:\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "It works by finding the best decision boundary (hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "Concept:\n",
        "\n",
        "- In SVM, each data item is plotted as a point in an n-dimensional space (where n is the number of features).\n",
        "\n",
        "- The goal is to find a hyperplane that best separates the classes.\n",
        "\n",
        "For example:\n",
        "\n",
        "- In 2D, the hyperplane is a line.\n",
        "\n",
        "- In 3D, it is a plane.\n",
        "\n",
        "- In higher dimensions, it is called a hyperplane.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "    Support Vectors:\n",
        "\n",
        "The data points that lie closest to the hyperplane and influence its position and orientation.\n",
        "\n",
        "    Margin:\n",
        "The distance between the hyperplane and the nearest data points (support vectors).\n",
        "SVM maximizes this margin for better generalization.\n",
        "\n",
        "    Kernel Trick:\n",
        "When data is not linearly separable, SVM uses kernel functions to transform it into a higher-dimensional space where it becomes separable.\n",
        "- Common kernels:\n",
        "\n",
        "Linear Kernel\n",
        "\n",
        "Polynomial Kernel\n",
        "\n",
        "Radial Basis Function (RBF) Kernel\n",
        "\n",
        "Sigmoid Kernel\n",
        "\n",
        "- Types of SVM:\n",
        "\n",
        "1. Linear SVM:\n",
        "Used when data is linearly separable.\n",
        "\n",
        "2. Non-Linear SVM:\n",
        "Used when data cannot be separated by a straight line â€” uses kernel functions to separate data in higher dimensions."
      ],
      "metadata": {
        "id": "WjUJ4h1l57DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6.\n",
        "\n",
        "Ans\n",
        "\n",
        "Definition:\n",
        "\n",
        "The Kernel Trick is a mathematical technique used in Support Vector Machines (SVMs) to handle non-linearly separable data by transforming it into a higher-dimensional space where it becomes linearly separable, without explicitly performing the transformation.\n",
        "\n",
        "Concept:\n",
        "\n",
        "- In many real-world problems, data points cannot be separated by a straight line (or hyperplane) in their original feature space.\n",
        "\n",
        "- The idea of the kernel trick is to map the input data from its original space to a higher-dimensional feature space, where a separating hyperplane can be found.\n",
        "\n",
        " Advantages of the Kernel Trick:\n",
        "\n",
        "- Allows SVMs to efficiently solve non-linear classification problems.\n",
        "\n",
        "- Avoids the computational cost of explicitly transforming data into higher dimensions.\n",
        "\n",
        "- Provides flexibility to choose different kernels based on the problem.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Choosing the right kernel and tuning its parameters can be difficult.\n",
        "\n",
        "- For very large datasets, kernel computations can become computationally expensive."
      ],
      "metadata": {
        "id": "EUo7z19U57AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7.\n",
        "\n",
        "Ans\n"
      ],
      "metadata": {
        "id": "-BHh_OSF569q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSKeY23P5qa5",
        "outputId": "4d898761-6529-439d-ee88-491247ed75cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Linear Kernel: 98.15 %\n",
            "Accuracy using RBF Kernel: 75.93 %\n",
            "\n",
            "The Linear kernel performs better on this dataset.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create two SVM classifiers with different kernels\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both models\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy using Linear Kernel:\", round(accuracy_linear * 100, 2), \"%\")\n",
        "print(\"Accuracy using RBF Kernel:\", round(accuracy_rbf * 100, 2), \"%\")\n",
        "\n",
        "# Compare which kernel performs better\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nThe Linear kernel performs better on this dataset.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nThe RBF kernel performs better on this dataset.\")\n",
        "else:\n",
        "    print(\"\\nBoth kernels perform equally well.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8.\n",
        "\n",
        "Ans\n",
        "\n",
        "    NaÃ¯ve Bayes Classifier\n",
        "Definition:\n",
        "\n",
        "The NaÃ¯ve Bayes classifier is a supervised machine learning algorithm based on Bayesâ€™ Theorem, used for classification tasks.\n",
        "\n",
        "It is called â€œnaÃ¯veâ€ because it assumes that all features (attributes) are independent of each other, which is rarely true in real-world data.\n",
        "\n",
        "Despite this simplifying assumption, NaÃ¯ve Bayes often performs very well in practice, especially for text classification, spam filtering, and sentiment analysis.\n",
        "\n",
        "The NaÃ¯ve Bayes classifier is a simple probabilistic model based on Bayesâ€™ Theorem.\n",
        "It is called â€œNaÃ¯veâ€ because it assumes that all features contribute independently to the outcome â€” an assumption that simplifies computation but is rarely true in real data."
      ],
      "metadata": {
        "id": "SpD5Tic-MLye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9.\n",
        "\n",
        "Ans\n",
        "\n",
        "NaÃ¯ve Bayes Classifier Variants: Gaussian, Multinomial, and Bernoulli\n",
        "\n",
        "The NaÃ¯ve Bayes classifier is a probabilistic supervised learning algorithm based on Bayesâ€™ Theorem. It assumes that all features are conditionally independent, which simplifies probability computations. Depending on the type of feature data, NaÃ¯ve Bayes has three common variants: Gaussian, Multinomial, and Bernoulli.\n",
        "\n",
        "    1. Gaussian NaÃ¯ve Bayes (GNB)\n",
        "\n",
        "Feature Type: Continuous numeric features.\n",
        "\n",
        "Assumption: Each feature is normally distributed (Gaussian distribution) within each class.\n",
        "\n",
        "Probability Calculation:\n",
        "For a continuous feature\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "\tâ€‹\n",
        " given class\n",
        "ğ¶\n",
        "C, the likelihood is computed using the Gaussian probability density function:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ğœ‹\n",
        "ğœ\n",
        "ğ¶\n",
        "2\n",
        "exp\n",
        "â¡\n",
        "(\n",
        "âˆ’\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğœ‡\n",
        "ğ¶\n",
        ")\n",
        "2\n",
        "2\n",
        "ğœ\n",
        "ğ¶\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)=\n",
        "2Ï€Ïƒ\n",
        "C\n",
        "2\n",
        "\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "exp(âˆ’\n",
        "2Ïƒ\n",
        "C\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’Î¼\n",
        "C\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "\n",
        "    2. Multinomial NaÃ¯ve Bayes (MNB)\n",
        "\n",
        "Feature Type: Discrete count-based features (non-negative integers).\n",
        "\n",
        "Assumption: Each feature represents the number of times an event occurs, such as word counts in documents.\n",
        "\n",
        "Probability Calculation:\n",
        "Multinomial NaÃ¯ve Bayes uses the multinomial distribution to calculate likelihoods:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "=\n",
        "(\n",
        "âˆ‘\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        ")\n",
        "!\n",
        "âˆ\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "!\n",
        "âˆ\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "P(Xâˆ£C)=\n",
        "âˆx\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "!\n",
        "(âˆ‘x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")!\n",
        "\tâ€‹\n",
        "\n",
        "âˆP(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)\n",
        "x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "    3. Bernoulli NaÃ¯ve Bayes (BNB)\n",
        "\n",
        "Feature Type: Binary features (0 or 1), indicating presence or absence of a characteristic.\n",
        "\n",
        "Assumption: Each feature is binary, modeled using a Bernoulli distribution.\n",
        "\n",
        "Probability Calculation:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "=\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "â‹…\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        ")\n",
        ")\n",
        "1\n",
        "âˆ’\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)=P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C)\n",
        "x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "â‹…(1âˆ’P(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£C))\n",
        "1âˆ’x\n",
        "i\n",
        "\tâ€‹\n"
      ],
      "metadata": {
        "id": "VtZEe9Ku2Ctq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10.\n",
        "\n",
        "Ans\n",
        "\n"
      ],
      "metadata": {
        "id": "mPYmn5rA2uS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create a Gaussian NaÃ¯ve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian NaÃ¯ve Bayes classifier:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "# Optional: Detailed evaluation\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD85D4vWMEGd",
        "outputId": "422de20c-0caf-40ce-a387-ad898c1ad5c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian NaÃ¯ve Bayes classifier: 94.15 %\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 57   6]\n",
            " [  4 104]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hek60MVX232e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}